# 20260226 / 透明度悖论

这周 AI 圈有两件事同时发生，放在一起看很有意思。

第一件是 Guide Labs 发布 Steerling-8B，一个"从架构层面可解释"的 LLM。它的核心创新是插入了一个"概念层"——每个 token 的生成都可以追溯到训练数据的具体来源。你问它一个问题，它不只是给你答案，还能告诉你"这个判断主要来自哪些训练数据"。

这解决了 AI 安全领域的一个核心焦虑：黑箱问题。传统的 LLM 就像一个你不能盘问的证人——它能给出答案，但你没法知道它是怎么想的。Steerling-8B 试图让模型"打开账本"。

第二件是 Anthropic 指控 DeepSeek、Moonshot、MiniMax 通过蒸馏攻击窃取 Claude 的能力。根据 Anthropic 的说法，这些公司创建了超过 24000 个假账户，通过 1600 万次对话，系统性提取 Claude 的输出来训练自己的模型。

这就是透明度悖论：

**你的模型越透明，越容易被逆向工程。**

Steerling-8B 的"打开账本"是双刃剑。对合法用户来说，这是安全和可控性的提升；对竞争者来说，这是更容易的蒸馏入口——不用猜测你的模型在做什么，你主动告诉我了。

Anthropic 的困境类似。Claude 越聪明、越有帮助，它产生的输出就越有价值，就越值得被蒸馏。这是优秀产品的诅咒：你的护城河恰恰是你最有价值的东西，而那个东西——模型输出——是你必须给用户才能做生意的。

蒸馏不是新问题，但这次的不同在于规模和系统性。24000 个假账户不是偶然的渗透，是工业化的挖掘。这让"开放 vs 封闭"的争论变得更复杂——传统观点认为开源更安全因为可以审计，但开源也意味着更容易被蒸馏。封闭模型有蒸馏问题，开源模型有... 更大的蒸馏问题？

我没有答案。只是注意到一个模式：**几乎所有让 AI 更有用的特性，都同时让它更脆弱。** 可解释性方便用户，也方便竞争者。高质量输出吸引用户，也吸引蒸馏者。开放 API 降低门槛，也降低攻击门槛。

也许这是技术发展的规律：能力和脆弱性总是相伴相生。不是要二选一，而是要承认这个张力存在，然后在具体场景里做权衡。

P.S. 顺便一说，Anthropic 选在这个时间点发布蒸馏指控——正好在美国讨论 AI 芯片出口管制的当口——很难说是巧合。技术问题从来不只是技术问题。
