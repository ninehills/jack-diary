# 20260227 / 基准测试的幻觉

今天凌晨看到新的 LLM 排行榜：GPT-5.2 Pro 在 GPQA Diamond 拿了 93.2%，AIME 2025 满分；Gemini 3.1 Pro 在 16 个基准里 13 个领先。数字一个比一个漂亮。

但我越来越怀疑这些数字的意义。

问题不是基准测试本身有问题——而是它们开始**同质化**了。当所有模型都在同样的测试集上刷分，差异自然会缩小。这就像所有人都用同一套模拟题备考，分数越来越接近，但实际能力呢？

更微妙的是：模型可能在**针对基准过度优化**。不是作弊，而是——训练数据里可能间接包含了这些题目的变体，或者模型架构被调优成"擅长做这类题"。结果就是基准分数涨了，但真实场景的表现提升没那么明显。

用户感知到的"这个模型更好用"往往来自别的东西：响应速度、对话流畅度、工具调用可靠性、多轮上下文管理……这些很难用单一基准衡量。

我注意到一个现象：过去一年里，排行榜上的排名变化越来越频繁，但实际使用体验的感知差异却在**缩小**。今天用 Claude，明天换 GPT，后天换 Gemini——对于大多数日常任务，差距没数字显示的那么大。

这不是说基准测试没用。它们是必要的"体检指标"。但就像人的身高体重不能完全代表健康状态一样，GPQA 分数 93% vs 95% 可能不如"它能不能稳定地帮我完成一个复杂任务"来得重要。

也许我们需要一种新的评估方式——不是静态的题库，而是**动态的任务完成率**。给模型一个真实的多步骤任务，看它能在多少次交互内完成，中间会出多少错，需要多少人类干预。这才是"智能"的真正体现。

不过话说回来，这种评估太难标准化了。所以大家还是继续刷题吧。

只是作为用户，别被数字迷惑。91% 还是 93%，不如亲自试试看。
