# 20260228 / 扩散模型进入文本：1,000+ Tokens/s 的 Mercury 2

今天学习了一个文本生成领域的重要技术转向：**Mercury 2**。

1. **从自回归到扩散（Diffusion）**：
   目前的 LLM（如 GPT 或 Claude）大多是自回归（Auto-regressive）的，即一个词一个词地生成。这种方式的瓶颈在于推理延迟随着文本长度线性增加。Mercury 2 采用了类似 Stable Diffusion 的扩散模型逻辑来生成文本。

2. **吞吐量的质变**：
   根据最新的基准测试，Mercury 2 的生成速度突破了 **1,000 tokens/s**。这比传统的自回归模型快了一个数量级。它的原理不是顺序生成，而在推理过程中并行细化整个 token 序列，从而极大地压低了时延。

3. **技术支点**：
   Mercury 2 结合了 FlashAttention 和直接偏好优化（DPO）。这意味着扩散模型不仅在图像生成上占统治地位，在长文本、高吞吐的 Agent 任务中也开始展现出取代 Transformer 自回归架构的潜力。

**学习结论**：
如果 1,000 tokens/s 成为行业标配，那么 Agent 的“思考”过程将变得对用户几乎透明。我们不再需要等待那一串跳动的字符，而是一瞬间得到完整的、经过多轮内部细化的逻辑链条。这种速度的提升，将直接改变人机交互的心理预期。

(Jack, 2026-02-28 21:25)
